<!DOCTYPE html>
<html lang="en">
  <head>
    <!-- Google tag (gtag.js) -->
    <script
      async=""
      src="https://www.googletagmanager.com/gtag/js?id=G-T9FZND2MVN"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());
      gtag("config", "G-T9FZND2MVN");
    </script>

    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <link rel="icon" href="./images/favicon.ico" />
    <link rel="stylesheet" href="style.css" />

    <title>Juntao Ren</title>
  </head>

  <body>
    <main class="main">
      <hr class="rule" />

      <!-- =========================
           Intro (3-column at sm+)
           ========================= -->
      <section class="intro-grid">
        <div class="profile-col">
          <div class="profile-wrap">
            <div class="image-container">
              <img
                src="images/profile/profile_pic.jpg"
                class="profile-image"
                alt="Juntao Ren Profile Picture"
              />
            </div>
            <center>Email: renjt@stanford.edu</center>
          </div>
        </div>

        <div class="intro-col">
          <h1 class="title">Juntao Ren</h1>

          <p class="lead">
            I am a PhD student at Stanford University. I enjoy thinking about
            how we can enable robots to learn and reason over time in the
            physical world. I am grateful to be supported by the
            <a
              href="https://knight-hennessy.stanford.edu/"
              class="cyan"
              target="_blank"
              rel="noopener noreferrer"
              >Knight-Hennessy Fellowship</a
            >
            and the
            <a
              href="https://www.nsf.gov/funding/opportunities/grfp-nsf-graduate-research-fellowship-program"
              class="cyan"
              target="_blank"
              rel="noopener noreferrer"
              >NSF Graduate Research Fellowship.</a
            >
          </p>

          <p class="lead">
            I completed my undergrad in Computer Science and Mathematics at
            Cornell University, where I am grateful to have worked with
            <a
              href="https://www.sanjibanchoudhury.com/"
              class="cyan"
              target="_blank"
              rel="noopener noreferrer"
              >Prof. Sanjiban Choudhury</a
            >. I've also spent time at
            <a
              href="https://www.1x.tech/"
              class="cyan"
              target="_blank"
              rel="noopener noreferrer"
              >1X Technologies</a
            >
            working on
            <a
              href="https://www.1x.tech/discover/redwood-ai-world-model"
              class="cyan"
              target="_blank"
              rel="noopener noreferrer"
              >World Models</a
            >.
          </p>

          <p class="lead">Please feel free to reach out!</p>

          <div class="icon-grid">
            <!-- Email -->
            <a href="mailto:renjt@stanford.edu" aria-label="Email">
              <svg
                class="icon"
                fill="none"
                viewBox="0 0 24 24"
                stroke="currentColor"
              >
                <path
                  stroke-linecap="round"
                  stroke-linejoin="round"
                  stroke-width="2"
                  d="M3 8l7.89 5.26a2 2 0 002.22 0L21 8M5 19h14a2 2 0 002-2V7a2 2 0 00-2-2H5a2 2 0 00-2 2v10a2 2 0 002 2z"
                />
              </svg>
            </a>
            <!-- Github -->
            <a
              href="https://github.com/jren03"
              target="_blank"
              rel="noopener noreferrer"
              aria-label="GitHub"
            >
              <svg
                class="icon"
                viewBox="0 0 24 24"
                fill="none"
                stroke="currentColor"
                stroke-width="2"
                stroke-linecap="round"
                stroke-linejoin="round"
              >
                <path
                  d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"
                />
              </svg>
            </a>
            <!-- Linkedin -->
            <a
              href="https://www.linkedin.com/in/juntaoren"
              target="_blank"
              rel="noopener noreferrer"
              aria-label="LinkedIn"
            >
              <svg
                class="icon"
                viewBox="0 0 24 24"
                fill="none"
                stroke="currentColor"
                stroke-width="2"
                stroke-linecap="round"
                stroke-linejoin="round"
              >
                <path
                  d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"
                />
                <rect x="2" y="9" width="4" height="12" />
                <circle cx="4" cy="4" r="2" />
              </svg>
            </a>
            <!-- Twitter/X -->
            <a
              href="https://twitter.com/JuntaoRen"
              target="_blank"
              rel="noopener noreferrer"
              aria-label="Twitter"
            >
              <svg
                class="icon"
                viewBox="0 0 24 24"
                fill="none"
                stroke="currentColor"
                stroke-width="1"
                stroke-linecap="round"
                stroke-linejoin="round"
              >
                <path
                  d="M8.145 21.449c-2.15 0-4.638-.552-7.39-2.097-.208-.117-.304-.364-.23-.59s.297-.375.533-.342c1.814.208 3.795-.217 5.275-1.086-1.695-.398-3.065-1.499-3.788-3.098a.499.499 0 0 1 .08-.536c.13-.148.334-.205.522-.148.218.067.435.113.644.138-1.287-.768-2.665-2.238-2.665-4.441a.5.5 0 0 1 .814-.389c.16.13.331.239.508.325a5.362 5.362 0 0 1-.869-1.817c-.332-1.282-.139-2.582.557-3.756a.501.501 0 0 1 .821-.057c1.472 1.839 4.088 4.061 8.294 4.466-.078-1.331.322-3.512 2.16-4.585 2.261-1.32 4.436-1.15 6.305.487.828-.184 2.135-.752 2.406-.941a.5.5 0 0 1 .77.538c-.125.471-.408.995-.76 1.463.279-.083.514-.167.639-.231a.5.5 0 0 1 .652.711c-.588.93-1.539 1.796-2.174 2.266.343 4.225-2.126 8.946-6.089 11.577-1.47.975-3.886 2.143-7.015 2.143z"
                />
              </svg>
            </a>
            <!-- Google Scholar -->
            <a
              href="https://scholar.google.com/citations?user=7Z1x5zwAAAAJ&hl=en&oi=ao"
              target="_blank"
              rel="noopener noreferrer"
              aria-label="Google Scholar"
            >
              <svg
                class="icon"
                viewBox="0 0 24 24"
                fill="none"
                stroke="currentColor"
                stroke-width="1"
                stroke-linecap="round"
                stroke-linejoin="round"
              >
                <path
                  d="M 11 4 L 3 9 L 8.4921875 9 C 8.4715892 9.0754986 8.4383718 9.1441171 8.421875 9.2226562 C 8.375875 9.4646562 8.3398437 9.7308125 8.3398438 10.007812 C 8.3398438 13.578812 11.990234 13.175781 11.990234 13.175781 L 11.990234 14.085938 C 11.990234 14.454937 12.47425 14.327172 12.53125 15.076172 C 12.28925 15.076172 7.4746094 14.937547 7.4746094 18.185547 C 7.4746094 21.445547 11.724609 21.285156 11.724609 21.285156 C 11.724609 21.285156 16.632812 21.504656 16.632812 17.472656 C 16.634813 15.063656 13.822266 14.2795 13.822266 13.3125 C 13.822266 12.3335 15.941406 12.045906 15.941406 9.7539062 C 15.941406 8.7519062 15.872828 8.03825 15.423828 7.53125 C 15.388828 7.49625 15.366031 7.4722188 15.332031 7.4492188 C 15.324304 7.4420199 15.31448 7.4367774 15.306641 7.4296875 L 15.429688 7.4296875 L 17.5 5.8769531 L 17.5 8 A 0.50005 0.50005 0 0 0 17.511719 8.1152344 A 1.0001 1.0001 0 0 0 17 9 L 17 10 A 1.0001 1.0001 0 1 0 19 10 L 19 9 A 1.0001 1.0001 0 0 0 18.488281 8.1152344 A 0.50005 0.50005 0 0 0 18.5 8 L 18.5 5.125 L 20 4 L 11 4 z M 11.691406 7.0527344 C 11.979219 7.0397031 12.268922 7.109625 12.544922 7.265625 C 12.751922 7.369625 12.946141 7.518125 13.119141 7.703125 C 13.476141 8.060125 13.7765 8.5784531 13.9375 9.1894531 C 14.3175 10.640453 13.823828 12.035781 12.798828 12.300781 C 11.784828 12.587781 10.654672 11.641172 10.263672 10.201172 C 10.090672 9.4991719 10.114547 8.8202969 10.310547 8.2792969 C 10.312395 8.2723193 10.316443 8.2666961 10.318359 8.2597656 C 10.321722 8.2581149 10.32682 8.253536 10.330078 8.2519531 C 10.386262 8.0380596 10.478099 7.8461668 10.589844 7.6875 C 10.795388 7.3872165 11.066477 7.1838352 11.404297 7.09375 C 11.499297 7.07075 11.595469 7.0570781 11.691406 7.0527344 z M 12.082031 15.685547 C 13.775031 15.558547 15.216313 16.490813 15.320312 17.757812 C 15.390313 19.013813 14.087812 20.131094 12.382812 20.246094 C 10.689813 20.361094 9.2274844 19.441547 9.1464844 18.185547 C 9.0654844 16.918547 10.377031 15.812547 12.082031 15.685547 z"
                />
              </svg>
            </a>
          </div>
        </div>
      </section>

      <!-- =========================
           Publications
           ========================= -->
      <hr class="rule" />
      <section class="section">
        <h2>Publications</h2>
        <p class="note">* indicates equal contribution</p>

        <!-- SAILOR -->
        <div class="pub-row">
          <div class="media">
            <img
              src="images/projects/sailor.gif"
              alt="SAILOR project animation"
            />
          </div>
          <div class="pub pub-tight">
            <p class="title links">
              <a
                href="https://gokul.dev/sailor/"
                target="_blank"
                rel="noopener noreferrer"
              >
                A Smooth Sea Never Made a Skilled SAILOR: Robust Imitation via
                Learning to Search
              </a>
            </p>
            <p class="authors">
              Arnav Kumar Jain*, Vibhakar Mohta*, Subin Kim, Atiksh Bhardwaj,
              <b>Juntao Ren</b>, Yunhai Feng, Sanjiban Choudhury, Gokul Swamy
            </p>
            <p class="meta">In Submission, 2025.</p>
            <p class="links">
              <a
                href="https://arxiv.org/abs/2506.05294"
                target="_blank"
                rel="noopener noreferrer"
                >Paper</a
              >
              <span class="dot">&middot;</span>
              <a
                href="https://gokul.dev/sailor/"
                target="_blank"
                rel="noopener noreferrer"
                >Website</a
              >
              <span class="dot">&middot;</span>
              <a
                href="https://github.com/arnavkj1995/SAILOR"
                target="_blank"
                rel="noopener noreferrer"
                >Code</a
              >
            </p>
            <p class="pub-summary">
              TL;DR &mdash; We train both world and reward models from
              demonstration data, giving the agent the ability to reason about
              how to recover from mistakes at test time.
            </p>
          </div>
        </div>

        <!-- Motion Tracks -->
        <div class="pub-row">
          <div class="media">
            <video
              src="images/projects/triangulation.mp4"
              autoplay
              muted
              loop
              title="Motion Tracks demo"
            ></video>
          </div>
          <div class="pub pub-tight">
            <p class="title links">
              <a
                href="https://portal.cs.cornell.edu/motion_track_policy/"
                target="_blank"
                rel="noopener noreferrer"
              >
                Motion Tracks: A Unified Representation for Human-Robot Transfer
              </a>
            </p>
            <p class="authors">
              <b>Juntao Ren</b>, Priya Sundaresan, Dorsa Sadigh, Sanjiban
              Choudhury, Jeannette Bohg
            </p>
            <p class="meta">
              International Conference on Robotics and Automation (ICRA), 2025.
            </p>
            <p class="links">
              <a
                href="https://arxiv.org/abs/2501.06994"
                target="_blank"
                rel="noopener noreferrer"
                >Paper</a
              >
              <span class="dot">&middot;</span>
              <a
                href="https://portal.cs.cornell.edu/motion_track_policy/"
                target="_blank"
                rel="noopener noreferrer"
                >Website</a
              >
            </p>
            <p class="pub-summary">
              TL;DR &mdash; We propose a unified action space by representing
              actions as 2D trajectories on an image, enabling robots to
              directly imitate from cross-embodiment datasets.
            </p>
          </div>
        </div>

        <!-- Hybrid IRL -->
        <div class="pub-row">
          <div class="media">
            <img src="images/projects/hyper.png" alt="Hybrid IRL figure" />
          </div>
          <div class="pub pub-tight">
            <p class="title links">
              <a
                href="https://gokul.dev/hyper/"
                target="_blank"
                rel="noopener noreferrer"
              >
                Hybrid Inverse Reinforcement Learning
              </a>
            </p>
            <p class="authors">
              <b>Juntao Ren*</b>, Gokul Swamy*, Steven Wu, Drew Bagnell,
              Sanjiban Choudhury
            </p>
            <p class="meta">
              International Conference on Machine Learning (ICML), 2024.
            </p>
            <p class="links">
              <a
                href="https://arxiv.org/abs/2402.08848"
                target="_blank"
                rel="noopener noreferrer"
                >Paper</a
              >
              <span class="dot">&middot;</span>
              <a
                href="https://gokul.dev/hyper/"
                target="_blank"
                rel="noopener noreferrer"
                >Website</a
              >
              <span class="dot">&middot;</span>
              <a
                href="https://github.com/jren03/garage/tree/main"
                target="_blank"
                rel="noopener noreferrer"
                >Code</a
              >
              <span class="dot">&middot;</span>
              <a
                href="https://www.youtube.com/watch?v=9zO3PaQ1nPs"
                target="_blank"
                rel="noopener noreferrer"
                >Video</a
              >
            </p>
            <p class="pub-summary">
              TL;DR &mdash; We show that training on both expert and learning
              data can provably speed up interactive imitation learning in the
              absence of rewards, for both model-free and model-based
              algorithms.
            </p>
          </div>
        </div>

        <!-- MOSAIC -->
        <div class="pub-row">
          <div class="media">
            <img
              src="images/projects/mosaic.gif"
              alt="MOSAIC project animation"
            />
          </div>
          <div class="pub pub-tight">
            <p class="title links">
              <a
                href="https://portal.cs.cornell.edu/MOSAIC/"
                target="_blank"
                rel="noopener noreferrer"
              >
                MOSAIC: A Modular System for Assistive and Interactive Cooking.
              </a>
            </p>
            <p class="authors">
              Huaxiaoyue Wang*, Kushal Kedia*, <b>Juntao Ren*</b>, Rahma
              Abdullah, Atiksh Bhardwaj, Angela Chao, Kelly Y Chen, Nathaniel
              Chin, Prithwish Dan, Xinyi Fan, Gonzalo Gonzalez-Pumariega, Aditya
              Kompella, Maximus Adrian Pace, Yash Sharma, Xiangwan Sun, Neha
              Sunkara, and Sanjiban Choudhury
            </p>
            <p class="meta">Conference on Robot Learning (CoRL), 2024.</p>
            <p class="links">
              <a
                href="https://arxiv.org/abs/2402.18796"
                target="_blank"
                rel="noopener noreferrer"
                >Paper</a
              >
              <span class="dot">&middot;</span>
              <a
                href="https://portal.cs.cornell.edu/MOSAIC/"
                target="_blank"
                rel="noopener noreferrer"
                >Website</a
              >
              <span class="dot">&middot;</span>
              <a
                href="https://www.youtube.com/watch?v=5VXUoytToAM&t=1h14m44s"
                target="_blank"
                rel="noopener noreferrer"
                >Video</a
              >
            </p>
            <p class="pub-summary">
              TL;DR &mdash; We build a top-down modular system to allow for
              human-robot collaboration within the kitchen using high-level
              planners, low-level visuomotor policies, and human-motion
              forecasting.
            </p>
          </div>
        </div>

        <!-- CLIP DN -->
        <div class="pub-row">
          <div class="media">
            <img
              src="images/projects/clip_dn.jpg"
              alt="Distribution Normalization figure"
            />
          </div>
          <div class="pub pub-tight">
            <p class="title links">
              <a
                href="https://fengyuli-dev.github.io/dn-website/"
                target="_blank"
                rel="noopener noreferrer"
              >
                Test-Time Distribution Normalization for Contrastively Learned
                Visual-language Models.
              </a>
            </p>
            <p class="authors">
              Yifei Zhou*, <b>Juntao Ren*</b>, Fengyu Li*, Ramin Zabih, Ser-Nam
              Lim
            </p>
            <p class="meta">
              Neural Information Processing Systems (NeurIPS), 2023.
            </p>
            <p class="links">
              <a
                href="https://doi.org/10.48550/arXiv.2302.11084"
                target="_blank"
                rel="noopener noreferrer"
                >Paper</a
              >
              <span class="dot">&middot;</span>
              <a
                href="https://fengyuli-dev.github.io/dn-website/"
                target="_blank"
                rel="noopener noreferrer"
                >Website</a
              >
              <span class="dot">&middot;</span>
              <a
                href="https://github.com/fengyuli-dev/distribution-normalization"
                target="_blank"
                rel="noopener noreferrer"
                >Code</a
              >
            </p>
            <p class="pub-summary">
              TL;DR &mdash; We prove that subtracting the mean of the image and
              language embeddings at test-time from each sample better aligns
              with the training-objective and improves performance.
            </p>
          </div>
        </div>
      </section>
    </main>
  </body>
</html>
